#!/usr/bin/env python3
"""
ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½¿ç”¨æ–¹æ³•: python download_model.py <ãƒ¢ãƒ‡ãƒ«å>
ä¾‹: python download_model.py Qwen/Qwen2.5-VL-3B-Instruct
"""

import os
import sys
import argparse
import time
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForVision2Seq, AutoModel, AutoConfig
from huggingface_hub import snapshot_download
import torch

def show_usage():
    """ä½¿ç”¨æ–¹æ³•ã‚’è¡¨ç¤º"""
    print("ä½¿ç”¨æ–¹æ³•: python download_model.py <ãƒ¢ãƒ‡ãƒ«å>")
    print("")
    print("ä¾‹:")
    print("ğŸ”¤ ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰:")
    print("  python download_model.py Qwen/Qwen2.5-0.5B-Instruct")
    print("  python download_model.py Qwen/Qwen2.5-1.5B-Instruct")
    print("  python download_model.py Qwen/Qwen2.5-3B-Instruct")
    print("")
    print("ğŸ–¼ï¸ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«:")
    print("  python download_model.py Qwen/Qwen2.5-VL-3B-Instruct-AWQ")
    print("  python download_model.py Qwen/Qwen2-VL-1.5B-Instruct")
    print("")
    print("ğŸ“ ãã®ä»–:")
    print("  python download_model.py microsoft/DialoGPT-medium")
    print("")
    print("æ³¨æ„: ãƒ¢ãƒ‡ãƒ«åã¯æ­£ã—ã„Hugging Faceã®ãƒªãƒã‚¸ãƒˆãƒªIDã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")

def get_directory_size(directory):
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç·ã‚µã‚¤ã‚ºã‚’è¨ˆç®—"""
    total_size = 0
    file_count = 0
    
    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.isfile(file_path):
                total_size += os.path.getsize(file_path)
                file_count += 1
    
    return total_size, file_count

def download_model_with_progress(model_name, download_dir):
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦é€²æ—ã‚’è¡¨ç¤º"""
    print(f"ğŸ” ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ä¸­: {model_name}")
    
    try:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        print("ğŸ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆä¸­...")
        os.makedirs(download_dir, exist_ok=True)
        print(f"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {download_dir}")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print("ğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        config = AutoConfig.from_pretrained(model_name, cache_dir=download_dir)
        print(f"âœ… è¨­å®šå®Œäº†: {config.model_type}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=download_dir)
        print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®Œäº†")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        print("ğŸ§  ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print("â³ åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
        
        # æ±ç”¨çš„ãªAutoModelã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•åˆ¤å®š
        print("ğŸ” ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤å®šä¸­...")
        model = AutoModel.from_pretrained(
            model_name, 
            cache_dir=download_dir,
            torch_dtype='auto',
            device_map='auto'
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å®Œäº†")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        model_path = os.path.join(download_dir, 'models--' + model_name.replace('/', '--'))
        if os.path.exists(model_path):
            total_size, file_count = get_directory_size(model_path)
            
            print(f"ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµ±è¨ˆ:")
            print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
            print(f"   - ç·ã‚µã‚¤ã‚º: {total_size / (1024**3):.2f} GB")
        
        print("ğŸ‰ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰')
    parser.add_argument('model_name', nargs='?', help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: Qwen/Qwen2.5-VL-3B-Instructï¼‰')
    parser.add_argument('--download-dir', default='./hf_models', help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./hf_modelsï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
    if not args.model_name:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        show_usage()
        sys.exit(1)
    
    print("ğŸš€ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model_name}")
    print(f"ä¿å­˜å…ˆ: {args.download_dir}")
    print("")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    success = download_model_with_progress(args.model_name, args.download_dir)
    
    if success:
        print("")
        print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("")
        print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"1. vLLMã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•:")
        print(f"   ./run_vllm_quantized.sh {args.model_name} none")
        print("")
        print("2. é‡å­åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ:")
        print(f"   ./run_vllm_quantized.sh {args.model_name} bitsandbytes")
        print("")
        print("3. ã¾ãŸã¯ã€åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:")
        print("   python download_model.py <åˆ¥ã®ãƒ¢ãƒ‡ãƒ«å>")
    else:
        print("")
        print("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        print("ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("1. ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ã„ã‹")
        print("2. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå®‰å®šã—ã¦ã„ã‚‹ã‹")
        print("3. ååˆ†ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒã‚ã‚‹ã‹")
        sys.exit(1)

if __name__ == "__main__":
    main() 