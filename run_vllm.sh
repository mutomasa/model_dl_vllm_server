#!/bin/bash

# ================================
# ãƒ¢ãƒ‡ãƒ«åã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å–å¾—
# ================================
MODEL_NAME=$1

if [ -z "$MODEL_NAME" ]; then
  echo "ãƒ¢ãƒ‡ãƒ«åã‚’å¼•æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„"
  echo "ä½¿ç”¨ä¾‹: ./run_vllm.sh Qwen/Qwen3-8B"
  echo "ä½¿ç”¨ä¾‹: ./run_vllm.sh meta-llama/Llama-2-7b-chat-hf"
  exit 1
fi

PORT=8000
HOST="localhost"

# ================================
# HFãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿…è¦ãªã‚‰ï¼‰
# ================================
# export HUGGINGFACE_TOKEN="your_token"
# huggingface-cli login --token "$HUGGINGFACE_TOKEN"

# ================================
# ãƒ¢ãƒ‡ãƒ«APIã‚µãƒ¼ãƒã‚’èµ·å‹•ï¼ˆOpenAIäº’æ›APIï¼‰
# ================================
echo "ğŸš€ vLLMã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ä¸­..."
echo "ãƒ¢ãƒ‡ãƒ«: $MODEL_NAME"
echo "URL: http://$HOST:$PORT"
echo "APIå½¢å¼: OpenAIäº’æ›"

# uvã‚’ä½¿ç”¨ã—ã¦vLLMã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
uv run python -m vllm.entrypoints.openai.api_server \
  --model "$MODEL_NAME" \
  --trust-remote-code \
  --port $PORT \
  --host $HOST \
  --download-dir ./hf_models &

# ================================
# ã‚µãƒ¼ãƒãŒç«‹ã¡ä¸ŠãŒã‚‹ã¾ã§å°‘ã—å¾…æ©Ÿ
# ================================
echo "ğŸ“¡ vLLM APIã‚µãƒ¼ãƒã‚’èµ·å‹•ä¸­..."
sleep 10

# ================================
# ã‚µãƒ¼ãƒãƒ¼ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
# ================================
echo "ğŸ” ã‚µãƒ¼ãƒãƒ¼ã®å¥å…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ä¸­..."

max_attempts=76  # æœ€å¤§7åˆ†å¾…æ©Ÿ
attempt=0

while [ $attempt -lt $max_attempts ]; do
  if curl -s http://$HOST:$PORT/v1/models > /dev/null; then
    echo "âœ… vLLMã‚µãƒ¼ãƒãƒ¼ãŒæ­£å¸¸ã«èµ·å‹•ã—ã¾ã—ãŸï¼"
    echo "ğŸ“¡ OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: http://$HOST:$PORT/v1"
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    echo "ğŸ§ª ã‚µãƒ¼ãƒãƒ¼ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­..."
    curl -X POST http://$HOST:$PORT/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "default",
        "messages": [
          {"role": "user", "content": "å¯Œå£«å±±ã®é«˜ã•ã¯ï¼Ÿ"}
        ],
        "max_tokens": 64,
        "temperature": 0.7
      }' 2>/dev/null | jq -r '.choices[0].message.content' 2>/dev/null || echo "ãƒ†ã‚¹ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
    
    echo ""
    echo "ğŸ“‹ ä½¿ç”¨æ–¹æ³•:"
    echo "1. åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Streamlitã‚¢ãƒ—ãƒªã‚’èµ·å‹•:"
    echo "   uv run streamlit run streamlit_app.py"
    echo ""
    echo "2. ã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã™ã‚‹ã«ã¯ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„"
    echo "   ã¾ãŸã¯: pkill -f 'vllm.entrypoints.openai.api_server'"
    
    # ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¾…æ©Ÿ
    wait
    break
  else
    attempt=$((attempt + 1))
    echo "â³ èµ·å‹•å¾…æ©Ÿä¸­... ($attempt/$max_attempts)"
    sleep 5
  fi
done

if [ $attempt -eq $max_attempts ]; then
  echo "âŒ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"
  pkill -f "vllm.entrypoints.openai.api_server"
  exit 1
fi 